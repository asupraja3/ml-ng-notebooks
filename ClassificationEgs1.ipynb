{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asupraja3/ml-ng-notebooks/blob/main/ClassificationEgs1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c9ea2b",
      "metadata": {
        "id": "07c9ea2b"
      },
      "source": [
        "\n",
        "# Optional Lab: Classification using Logistic Regression\n",
        "\n",
        "*Adapted, self-contained version inspired by Andrew Ng's Machine Learning Specialization.*  \n",
        "This notebook contrasts **regression** and **classification** and shows why **logistic regression** is a better choice for binary outcomes than using linear regression with an arbitrary threshold.\n",
        "\n",
        "**You will:**  \n",
        "1. Plot small 1D and 2D classification datasets.  \n",
        "2. Try **Linear Regression + threshold** on categorical targets (and see its limitations).  \n",
        "3. Train **Logistic Regression** and inspect probabilities and decision boundaries.  \n",
        "4. Compare results visually and with simple metrics.\n",
        "\n",
        "**Prereqs:** Basic NumPy, matplotlib, and scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21172b8",
      "metadata": {
        "id": "f21172b8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Setup ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Matplotlib: ensure plots render inline in Jupyter\n",
        "# %matplotlib inline  # Uncomment if needed in your environment\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22143237",
      "metadata": {
        "id": "22143237"
      },
      "source": [
        "\n",
        "## Classification Problems (Binary)\n",
        "\n",
        "**Examples:** spam vs not spam, fraudulent vs legitimate transaction, malignant vs benign tumor.  \n",
        "Outcomes are **discrete** (0/1), not continuous numbers.\n",
        "\n",
        "**Conventions:**  \n",
        "- We will use `y = 1` for the **positive** class (e.g., malignant).  \n",
        "- We will use `y = 0` for the **negative** class (e.g., benign).\n",
        "\n",
        "> In small teaching datasets, we often visualize class labels with different markers.  \n",
        "> In real projects, features can be high-dimensional and not easily plotted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e4dc0e1",
      "metadata": {
        "id": "0e4dc0e1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Toy datasets (match the lecture's spirit) ===\n",
        "\n",
        "# 1D dataset (tumor size -> malignant?)\n",
        "x_train = np.array([0., 1., 2., 3., 4., 5.])\n",
        "y_train = np.array([0, 0, 0, 1, 1, 1])\n",
        "\n",
        "# 2D dataset (two features -> label). Each row is a sample [x0, x1].\n",
        "x_train2 = np.array([\n",
        "    [0.5, 1.5],\n",
        "    [1.0, 1.0],\n",
        "    [1.5, 0.5],\n",
        "    [3.5, 0.5],\n",
        "    [2.0, 2.0],\n",
        "    [1.0, 2.5]\n",
        "])\n",
        "y_train2 = np.array([0, 0, 0, 1, 1, 1])\n",
        "\n",
        "print(\"1D shapes:\", x_train.shape, y_train.shape)\n",
        "print(\"2D shapes:\", x_train2.shape, y_train2.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25fc7d27",
      "metadata": {
        "id": "25fc7d27"
      },
      "source": [
        "\n",
        "## Visualizing the data\n",
        "\n",
        "Left: single feature vs class label.  \n",
        "Right: two features, where position indicates the input and marker indicates the class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "525341ae",
      "metadata": {
        "id": "525341ae"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Plot: single variable (x vs class) ---\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "pos = y_train == 1\n",
        "neg = y_train == 0\n",
        "ax.scatter(x_train[neg], y_train[neg], marker='o', label='y=0')\n",
        "ax.scatter(x_train[pos], y_train[pos], marker='x', label='y=1')\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"y\")\n",
        "ax.set_title(\"One variable plot\")\n",
        "ax.set_ylim(-0.2, 1.2)\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4702af16",
      "metadata": {
        "id": "4702af16"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Plot: two variables (x0 vs x1) ---\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "pos2 = y_train2 == 1\n",
        "neg2 = y_train2 == 0\n",
        "ax.scatter(x_train2[neg2, 0], x_train2[neg2, 1], marker='o', label='y=0')\n",
        "ax.scatter(x_train2[pos2, 0], x_train2[pos2, 1], marker='x', label='y=1')\n",
        "ax.set_xlabel(\"x0\")\n",
        "ax.set_ylabel(\"x1\")\n",
        "ax.set_title(\"Two variable plot\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b55795f4",
      "metadata": {
        "id": "b55795f4"
      },
      "source": [
        "\n",
        "## Linear Regression Approach (and its problem)\n",
        "\n",
        "You may try to treat `y ∈ {0,1}` as numbers and fit **linear regression**, then apply a **0.5 threshold**:\n",
        "- Predict a *continuous* value `ŷ` and classify as 1 if `ŷ ≥ 0.5`, else 0.\n",
        "- This can work **accidentally** on some datasets but fails to model probabilities in `[0,1]`.  \n",
        "- Predictions can go below 0 or above 1, and extrapolation can be misleading.\n",
        "\n",
        "Let's try it on the 1D dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10814c8c",
      "metadata": {
        "id": "10814c8c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Linear Regression + threshold on 1D ===\n",
        "X1 = x_train.reshape(-1, 1)\n",
        "linreg = LinearRegression().fit(X1, y_train)\n",
        "yhat_lr = linreg.predict(X1)\n",
        "\n",
        "print(\"Linear Regression coefficients:\", linreg.coef_, \"intercept:\", linreg.intercept_)\n",
        "print(\"Raw predictions (continuous):\", yhat_lr)\n",
        "\n",
        "# Apply 0.5 threshold to map to {0,1}\n",
        "yhat_cls = (yhat_lr >= 0.5).astype(int)\n",
        "print(\"Thresholded predictions:\", yhat_cls)\n",
        "print(\"Accuracy:\", accuracy_score(y_train, yhat_cls))\n",
        "print(confusion_matrix(y_train, yhat_cls))\n",
        "\n",
        "# Plot the fitted line and threshold\n",
        "xs = np.linspace(x_train.min()-0.5, x_train.max()+0.5, 200).reshape(-1, 1)\n",
        "ys = linreg.predict(xs)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "ax.scatter(x_train[y_train==0], y_train[y_train==0], marker='o', label='y=0')\n",
        "ax.scatter(x_train[y_train==1], y_train[y_train==1], marker='x', label='y=1')\n",
        "ax.plot(xs, ys, label='Linear fit')\n",
        "ax.axhline(0.5, linestyle='--', label='0.5 threshold')\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"prediction / label\")\n",
        "ax.set_title(\"Linear Regression + threshold on 1D\")\n",
        "ax.set_ylim(-0.2, 1.2)\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f354958c",
      "metadata": {
        "id": "f354958c"
      },
      "source": [
        "\n",
        "## Logistic Regression\n",
        "\n",
        "**Idea:** model \\( P(y=1 \\mid x) = \\sigma(w^T x + b) \\) where \\(\\sigma(z) = 1/(1+e^{-z})\\).  \n",
        "Outputs are **probabilities** in \\([0,1]\\), and the **decision rule** is usually \\(P ≥ 0.5\\).\n",
        "\n",
        "Let's fit logistic regression on the same 1D dataset and compare.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84d34fb",
      "metadata": {
        "id": "e84d34fb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Logistic Regression on 1D ===\n",
        "logreg_1d = LogisticRegression(solver='lbfgs')\n",
        "logreg_1d.fit(X1, y_train)\n",
        "proba_1d = logreg_1d.predict_proba(X1)[:, 1]\n",
        "yhat_log_1d = (proba_1d >= 0.5).astype(int)\n",
        "\n",
        "print(\"Logistic Coef:\", logreg_1d.coef_, \"Intercept:\", logreg_1d.intercept_)\n",
        "print(\"Train accuracy:\", accuracy_score(y_train, yhat_log_1d))\n",
        "print(confusion_matrix(y_train, yhat_log_1d))\n",
        "\n",
        "# Plot probability curve\n",
        "xs = np.linspace(x_train.min()-0.5, x_train.max()+0.5, 200).reshape(-1, 1)\n",
        "probs = logreg_1d.predict_proba(xs)[:, 1]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "ax.scatter(x_train[y_train==0], y_train[y_train==0], marker='o', label='y=0')\n",
        "ax.scatter(x_train[y_train==1], y_train[y_train==1], marker='x', label='y=1')\n",
        "ax.plot(xs, probs, label='P(y=1 | x)')\n",
        "ax.axhline(0.5, linestyle='--', label='0.5 threshold')\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"probability / label\")\n",
        "ax.set_title(\"Logistic Regression probability curve (1D)\")\n",
        "ax.set_ylim(-0.2, 1.2)\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "900d7936",
      "metadata": {
        "id": "900d7936"
      },
      "source": [
        "\n",
        "## 2D case and Decision Boundary\n",
        "\n",
        "We'll fit both models on the 2D dataset and visualize the **decision boundary**:\n",
        "- For **linear regression + threshold**, use the fitted plane and cut at 0.5.\n",
        "- For **logistic regression**, plot where \\(\\sigma(w^T x + b) = 0.5\\) which is the line \\(w^T x + b = 0\\).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc97050",
      "metadata": {
        "id": "ccc97050"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Helper to draw a linear boundary given coefficients and intercept at a desired level (0.5 by default)\n",
        "def plot_linear_boundary(ax, w, b, level=0.5, xlim=(0, 4), ylim=(0, 4), label=\"boundary\"):\n",
        "    # For a plane yhat = w0*x0 + w1*x1 + b, boundary for yhat=level is w0*x0 + w1*x1 + b = level\n",
        "    # Solve for x1 = -(w0*x0 + b - level)/w1  (if w1 != 0)\n",
        "    x0s = np.linspace(xlim[0], xlim[1], 100)\n",
        "    if abs(w[1]) < 1e-8:\n",
        "        return  # vertical-ish boundary; skip for simplicity\n",
        "    x1s = -(w[0]*x0s + b - level) / w[1]\n",
        "    ax.plot(x0s, x1s, linestyle='--', label=label)\n",
        "\n",
        "# Fit Linear Regression on 2D and derive thresholded classifier\n",
        "lin2 = LinearRegression().fit(x_train2, y_train2)\n",
        "print(\"2D Linear Regression w:\", lin2.coef_, \"b:\", lin2.intercept_)\n",
        "\n",
        "# Fit Logistic Regression on 2D\n",
        "log2 = LogisticRegression(solver='lbfgs')\n",
        "log2.fit(x_train2, y_train2)\n",
        "print(\"2D Logistic Regression w:\", log2.coef_[0], \"b:\", log2.intercept_[0])\n",
        "\n",
        "# Grid for background probabilities (logistic) to see the surface qualitatively\n",
        "x0_range = np.linspace(0, 4, 200)\n",
        "x1_range = np.linspace(0, 4, 200)\n",
        "xx0, xx1 = np.meshgrid(x0_range, x1_range)\n",
        "grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
        "probs2 = log2.predict_proba(grid)[:, 1].reshape(xx0.shape)\n",
        "\n",
        "# Scatter + boundaries\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "pos2 = y_train2 == 1\n",
        "neg2 = y_train2 == 0\n",
        "ax.scatter(x_train2[neg2, 0], x_train2[neg2, 1], marker='o', label='y=0')\n",
        "ax.scatter(x_train2[pos2, 0], x_train2[pos2, 1], marker='x', label='y=1')\n",
        "ax.set_xlabel(\"x0\")\n",
        "ax.set_ylabel(\"x1\")\n",
        "ax.set_title(\"2D Data with decision boundaries\")\n",
        "\n",
        "# optional: draw a few probability contours for logistic (0.2, 0.5, 0.8)\n",
        "cs = ax.contour(xx0, xx1, probs2, levels=[0.2, 0.5, 0.8], linestyles=[':', '--', ':'])\n",
        "ax.clabel(cs, fmt={0.2: '0.2', 0.5: '0.5', 0.8: '0.8'}, inline=True, fontsize=8)\n",
        "\n",
        "# Linear Regression + threshold boundary at yhat=0.5\n",
        "plot_linear_boundary(ax, lin2.coef_, lin2.intercept_, level=0.5, label=\"LinearReg @ 0.5\")\n",
        "\n",
        "# Logistic boundary at prob=0.5 => w^T x + b = 0\n",
        "w = log2.coef_[0]\n",
        "b = log2.intercept_[0]\n",
        "# For sigmoid(w·x+b)=0.5 => w·x + b = 0  -> x1 = -(w0*x0 + b)/w1\n",
        "plot_linear_boundary(ax, w, b, level=0.0, label=\"Logistic @ 0.5\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "737e99c0",
      "metadata": {
        "id": "737e99c0"
      },
      "source": [
        "\n",
        "## Quantitative comparison\n",
        "\n",
        "We'll compare accuracy on the training set (for small toy data this is fine).  \n",
        "In practice, use a train/validation split or cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7b412ef",
      "metadata": {
        "id": "b7b412ef"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compare predictions on 2D\n",
        "yhat_lr2 = (lin2.predict(x_train2) >= 0.5).astype(int)\n",
        "yhat_log2 = log2.predict(x_train2)\n",
        "\n",
        "print(\"Linear Regression + threshold (2D)\")\n",
        "print(\"  Acc:\", accuracy_score(y_train2, yhat_lr2))\n",
        "print(confusion_matrix(y_train2, yhat_lr2))\n",
        "print(classification_report(y_train2, yhat_lr2, digits=3))\n",
        "\n",
        "print(\"\\nLogistic Regression (2D)\")\n",
        "print(\"  Acc:\", accuracy_score(y_train2, yhat_log2))\n",
        "print(confusion_matrix(y_train2, yhat_log2))\n",
        "print(classification_report(y_train2, yhat_log2, digits=3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcf1f15e",
      "metadata": {
        "id": "dcf1f15e"
      },
      "source": [
        "\n",
        "## Takeaways\n",
        "\n",
        "- **Linear Regression + threshold** is not probabilistic and can extrapolate outside \\([0,1]\\).  \n",
        "- **Logistic Regression** directly models \\(P(y=1\\mid x)\\) with outputs in \\([0,1]\\).  \n",
        "- The **decision boundary** in logistic regression for threshold 0.5 is the set \\(w^T x + b = 0\\).  \n",
        "- For linearly separable data, a linear boundary can work well; for more complex patterns, consider **feature engineering** or **non-linear models** (e.g., kernels, trees, neural networks).\n",
        "\n",
        "**Suggested exercises:**\n",
        "1. Try shifting a few points and re-run to see how boundaries change.  \n",
        "2. Add polynomial features to the 2D data and fit logistic regression again.  \n",
        "3. Replace logistic regression with other classifiers (SVM, RandomForest) and compare.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}