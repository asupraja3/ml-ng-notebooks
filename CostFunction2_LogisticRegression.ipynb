{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asupraja3/ml-ng-notebooks/blob/main/CostFunction2_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "072ccbbd",
      "metadata": {
        "id": "072ccbbd"
      },
      "source": [
        "# Optional Lab: Cost Function for Logistic Regression\n",
        "\n",
        "**Goals**\n",
        "- Understand and implement the **logistic loss** for a single example.\n",
        "- Combine per-example losses into the overall **cost function** $J(w,b)$.\n",
        "- Build intuition by plotting a toy dataset and checking cost values for different parameters.\n",
        "\n",
        "> This notebook mirrors the structure and spirit of Andrew Ng’s optional lab and is fully executable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f69cddb3",
      "metadata": {
        "id": "f69cddb3"
      },
      "outputs": [],
      "source": [
        "# === Imports & plotting defaults ===\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (4, 4)\n",
        "plt.rcParams['font.size'] = 12\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8ce52f7",
      "metadata": {
        "id": "c8ce52f7"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We'll reuse the classic separable 2D toy dataset that appears across the decision boundary labs.\n",
        "\n",
        "- Features: $x = [x_0, x_1]$\n",
        "- Labels: $y \\in \\{0,1\\}$\n",
        "\n",
        "Below, we also define a small helper to plot the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a76e46",
      "metadata": {
        "id": "81a76e46"
      },
      "outputs": [],
      "source": [
        "# Toy dataset (m=6 examples, n=2 features)\n",
        "X = np.array([\n",
        "    [0.5, 1.5],\n",
        "    [1.0, 1.0],\n",
        "    [1.5, 0.5],\n",
        "    [3.0, 0.5],\n",
        "    [2.0, 2.0],\n",
        "    [1.0, 2.5]\n",
        "], dtype=float)\n",
        "\n",
        "y = np.array([0, 0, 0, 1, 1, 1], dtype=float)\n",
        "\n",
        "def plot_data(X, y, ax=None):\n",
        "    # Plot 2D points with different markers for y=0 vs y=1.\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(1,1, figsize=(4,4))\n",
        "    pos = y == 1\n",
        "    neg = y == 0\n",
        "    ax.scatter(X[neg,0], X[neg,1], marker='o', label='y=0')\n",
        "    ax.scatter(X[pos,0], X[pos,1], marker='x', label='y=1')\n",
        "    ax.set_xlim(0, 4)\n",
        "    ax.set_ylim(0, 4)\n",
        "    ax.set_xlabel('$x_0$')\n",
        "    ax.set_ylabel('$x_1$')\n",
        "    ax.legend(loc='upper right')\n",
        "    return ax\n",
        "\n",
        "# Quick preview\n",
        "ax = plot_data(X, y)\n",
        "ax.set_title('Toy dataset')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91feaed5",
      "metadata": {
        "id": "91feaed5"
      },
      "source": [
        "## Cost function recap\n",
        "\n",
        "For logistic regression, with parameters $w \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}$:\n",
        "- Linear score: $z^{(i)} = w^\\top x^{(i)} + b$\n",
        "- Sigmoid: $f_{w,b}(x^{(i)}) = \\sigma(z^{(i)}) = \\frac{1}{1 + e^{-z^{(i)}}}$\n",
        "\n",
        "**Logistic loss (single example)**\n",
        "$$\\text{loss}(f_{w,b}(x^{(i)}), y^{(i)}) = -y^{(i)}\\log(f_{w,b}(x^{(i)})) - (1-y^{(i)})\\log(1-f_{w,b}(x^{(i)}))$$\n",
        "\n",
        "**Cost over the dataset** (average loss across $m$ examples)\n",
        "$$J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m} \\text{loss}(f_{w,b}(x^{(i)}), y^{(i)})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fefbbe3",
      "metadata": {
        "id": "2fefbbe3"
      },
      "outputs": [],
      "source": [
        "# === Core functions ===\n",
        "def sigmoid(z):\n",
        "    # Numerically stable sigmoid\n",
        "    z = np.clip(z, -500, 500)\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def logistic_loss_single(y_i, yhat_i):\n",
        "    # Logistic loss for a single example\n",
        "    eps = 1e-15\n",
        "    yhat_i = np.clip(yhat_i, eps, 1 - eps)\n",
        "    return - (y_i * np.log(yhat_i) + (1 - y_i) * np.log(1 - yhat_i))\n",
        "\n",
        "def compute_cost_logistic(X, y, w, b):\n",
        "    # Average logistic cost J(w,b) over all m examples (loop version)\n",
        "    m, n = X.shape\n",
        "    cost_sum = 0.0\n",
        "    for i in range(m):\n",
        "        z_i = np.dot(X[i], w) + b\n",
        "        f_i = sigmoid(z_i)\n",
        "        cost_sum += logistic_loss_single(y[i], f_i)\n",
        "    return cost_sum / m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3b2e228",
      "metadata": {
        "id": "c3b2e228"
      },
      "source": [
        "## Sanity check\n",
        "\n",
        "Let's evaluate the cost for a simple parameter choice: $w=[1,1]$, $b=-3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ca53a1",
      "metadata": {
        "id": "b6ca53a1"
      },
      "outputs": [],
      "source": [
        "w = np.array([1.0, 1.0])\n",
        "b = -3.0\n",
        "cost = compute_cost_logistic(X, y, w, b)\n",
        "print('Cost for w=[1,1], b=-3  ->', cost)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0040950",
      "metadata": {
        "id": "a0040950"
      },
      "source": [
        "## Decision boundary intuition\n",
        "\n",
        "For any fixed $w$, changing $b$ shifts the decision boundary. Below we draw two lines (same slope, different intercepts) and compare their costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d93187ea",
      "metadata": {
        "id": "d93187ea"
      },
      "outputs": [],
      "source": [
        "def plot_lines_and_data(w, b_list):\n",
        "    ax = plot_data(X, y)\n",
        "    x0 = np.linspace(0, 4, 100)\n",
        "    for b in b_list:\n",
        "        if abs(w[1]) < 1e-12:\n",
        "            continue\n",
        "        x1 = -(w[0]*x0 + b) / w[1]\n",
        "        ax.plot(x0, x1, label=f'b={b:g}')\n",
        "    ax.set_title('Decision Boundary')\n",
        "    ax.legend(loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "w = np.array([1.0, 1.0])\n",
        "b1, b2 = -3.0, -4.0\n",
        "plot_lines_and_data(w, [b1, b2])\n",
        "print('Cost for b = -3 :', compute_cost_logistic(X, y, w, b1))\n",
        "print('Cost for b = -4 :', compute_cost_logistic(X, y, w, b2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5063a4e0",
      "metadata": {
        "id": "5063a4e0"
      },
      "source": [
        "## (Optional) Vectorized cost\n",
        "\n",
        "The same cost can be implemented without explicit Python loops for speed on large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76cf5e55",
      "metadata": {
        "id": "76cf5e55"
      },
      "outputs": [],
      "source": [
        "def compute_cost_logistic_vectorized(X, y, w, b):\n",
        "    z = X @ w + b\n",
        "    f = sigmoid(z)\n",
        "    eps = 1e-15\n",
        "    f = np.clip(f, eps, 1 - eps)\n",
        "    return np.mean(- (y * np.log(f) + (1 - y) * np.log(1 - f)))\n",
        "\n",
        "w = np.array([1.0, 1.0])\n",
        "b = -3.0\n",
        "print('Looped    :', compute_cost_logistic(X, y, w, b))\n",
        "print('Vectorized:', compute_cost_logistic_vectorized(X, y, w, b))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ade487",
      "metadata": {
        "id": "26ade487"
      },
      "source": [
        "## What to remember\n",
        "- **Loss** is per-example; **cost** averages losses over all examples.\n",
        "- You typically use **gradient descent** (or variants) to find $(w,b)$ that minimize the cost.\n",
        "- For fixed $w$, changing $b$ shifts the decision boundary—reflected in different costs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}