{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asupraja3/ml-ng-notebooks/blob/main/CostFunction_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4d81a9a",
      "metadata": {
        "id": "c4d81a9a"
      },
      "source": [
        "\n",
        "# Optional Lab: Logistic Regression — **Cost Function & Logistic Loss**\n",
        "\n",
        "In this ungraded lab, you will:\n",
        "- Examine why the **squared error** loss is not ideal for logistic regression\n",
        "- Implement the **logistic loss** for a single example\n",
        "- Build the **average logistic cost** J(w,b) over a dataset\n",
        "- Visualize surfaces of the cost vs parameters w, b\n",
        "- Confirm the code is executable end‑to‑end with simple toy data\n",
        "\n",
        "> Notation used in the lab follows Andrew Ng's ML course convention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cda7b8c8",
      "metadata": {
        "id": "cda7b8c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ======================\n",
        "# 1) Imports & Plot Style\n",
        "# ======================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 - needed for 3D plotting\n",
        "\n",
        "# Matplotlib defaults similar to the course notebooks\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['figure.figsize'] = (6, 4)\n",
        "plt.rcParams['axes.grid'] = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00e5d878",
      "metadata": {
        "id": "00e5d878"
      },
      "source": [
        "\n",
        "## Toy dataset\n",
        "\n",
        "We will use a 1‑D feature x and a binary label y in {0,1} similar to the course examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5b7880a",
      "metadata": {
        "id": "b5b7880a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================\n",
        "# 2) A simple training set\n",
        "# ========================\n",
        "# Feature values 0..5 and a threshold-like label split\n",
        "x_train = np.array([0, 1, 2, 3, 4, 5], dtype=float)\n",
        "y_train = np.array([0, 0, 0, 1, 1, 1], dtype=float)\n",
        "\n",
        "m = x_train.shape[0]  # number of training examples\n",
        "print(f\"m = {m}, x shape = {x_train.shape}, y shape = {y_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "769a021f",
      "metadata": {
        "id": "769a021f"
      },
      "source": [
        "\n",
        "## Sigmoid and model f_w,b(x)\n",
        "\n",
        "The logistic model outputs a probability via the sigmoid:\n",
        "g(z)=1/(1+exp(-z)),    f_w,b(x) = g(w*x + b).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "490a47a6",
      "metadata": {
        "id": "490a47a6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==============================\n",
        "# 3) Sigmoid and model prediction\n",
        "# ==============================\n",
        "def sigmoid(z):\n",
        "    \"\"\"Numerically-stable sigmoid.\"\"\"\n",
        "    # Clip to avoid overflow in exp for large |z|\n",
        "    z = np.clip(z, -500, 500)\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def f_wb(x, w, b):\n",
        "    \"\"\"Model prediction for a scalar feature x (or vectorized x).\"\"\"\n",
        "    return sigmoid(w * x + b)\n",
        "\n",
        "# quick sanity check\n",
        "print(\"sigmoid(0) =\", sigmoid(0.0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c534267",
      "metadata": {
        "id": "8c534267"
      },
      "source": [
        "\n",
        "## Loss functions\n",
        "\n",
        "### (A) Squared error (for contrast)\n",
        "Using squared error with a sigmoid model can produce a non‑convex cost surface in (w,b), which may hinder gradient descent.\n",
        "\n",
        "### (B) Logistic loss (preferred for classification)\n",
        "For a single example (x^(i), y^(i)) and model output f = f_w,b(x^(i)):\n",
        "  - if y=1:  loss = -log(f)\n",
        "  - if y=0:  loss = -log(1-f)\n",
        "A compact implementation is:  loss(f,y) = -( y*log(f) + (1-y)*log(1-f) ).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc2d9076",
      "metadata": {
        "id": "cc2d9076"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 4) Loss for one example and dataset costs\n",
        "# ==========================================\n",
        "def squared_error(f, y):\n",
        "    \"\"\"Squared error for a single example (for contrast).\"\"\"\n",
        "    return 0.5 * (f - y)**2\n",
        "\n",
        "def logistic_loss(f, y, eps=1e-12):\n",
        "    \"\"\"Logistic loss for a single example (numerically safe).\"\"\"\n",
        "    # Clip f to (eps, 1-eps) to avoid log(0)\n",
        "    f = np.clip(f, eps, 1 - eps)\n",
        "    return -(y * np.log(f) + (1 - y) * np.log(1 - f))\n",
        "\n",
        "def squared_error_cost(x, y, w, b):\n",
        "    \"\"\"Average squared error cost over the dataset using a logistic model.\"\"\"\n",
        "    f = f_wb(x, w, b)\n",
        "    return np.mean(squared_error(f, y))\n",
        "\n",
        "def logistic_cost(x, y, w, b):\n",
        "    \"\"\"Average logistic cost J(w,b) over the dataset.\"\"\"\n",
        "    f = f_wb(x, w, b)\n",
        "    return np.mean(logistic_loss(f, y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f3122a3",
      "metadata": {
        "id": "8f3122a3"
      },
      "source": [
        "\n",
        "## Visualizing the two logistic loss curves\n",
        "\n",
        "The logistic loss is piecewise—one curve for y=1 and one for y=0—both defined on f in (0,1).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b2d7f2",
      "metadata": {
        "id": "d5b2d7f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====================================\n",
        "# 5) Plot the two logistic loss curves\n",
        "# ====================================\n",
        "def plot_two_logistic_loss_curves():\n",
        "    f = np.linspace(1e-4, 1-1e-4, 400)\n",
        "    loss_y1 = -np.log(f)           # y = 1\n",
        "    loss_y0 = -np.log(1 - f)       # y = 0\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(f, loss_y1, label=\"y=1:  -log(f)\")\n",
        "    plt.plot(f, loss_y0, label=\"y=0:  -log(1 - f)\")\n",
        "    plt.xlabel(\"f = sigmoid(wx + b)\")\n",
        "    plt.ylabel(\"loss(f, y)\")\n",
        "    plt.title(\"Logistic Loss (two curves)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_two_logistic_loss_curves()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82e85e44",
      "metadata": {
        "id": "82e85e44"
      },
      "source": [
        "\n",
        "## Cost surfaces vs parameters (w,b)\n",
        "\n",
        "We now visualize:\n",
        "1. Squared error cost when used with a sigmoid model (for contrast)  \n",
        "2. Logistic cost J(w,b) (the right choice for logistic regression)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b332420",
      "metadata": {
        "id": "1b332420"
      },
      "outputs": [],
      "source": [
        "\n",
        "# =============================================\n",
        "# 6) 3D surface plots for cost vs (w, b) params\n",
        "# =============================================\n",
        "def surface(cost_fn, x, y, w_range, b_range, n=80):\n",
        "    \"\"\"Compute a grid of cost values over w,b ranges.\"\"\"\n",
        "    w_vals = np.linspace(w_range[0], w_range[1], n)\n",
        "    b_vals = np.linspace(b_range[0], b_range[1], n)\n",
        "    W, B = np.meshgrid(w_vals, b_vals)\n",
        "    Z = np.zeros_like(W)\n",
        "    for i in range(W.shape[0]):\n",
        "        for j in range(W.shape[1]):\n",
        "            Z[i, j] = cost_fn(x, y, W[i, j], B[i, j])\n",
        "    return W, B, Z\n",
        "\n",
        "def plot_surface(W, B, Z, title=\"Cost surface\", zlabel=\"Cost\"):\n",
        "    fig = plt.figure(figsize=(6, 4.8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.plot_surface(W, B, Z, linewidth=0, antialiased=True, cmap=\"viridis\")\n",
        "    ax.set_xlabel(\"w\")\n",
        "    ax.set_ylabel(\"b\")\n",
        "    ax.set_zlabel(zlabel)\n",
        "    ax.set_title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---- Squared error with logistic model (for contrast) ----\n",
        "W, B, Z = surface(squared_error_cost, x_train, y_train, w_range=(-5, 15), b_range=(-10, 10), n=70)\n",
        "plot_surface(W, B, Z, title='\"Logistic\" Squared Error Cost vs (w, b)', zlabel=\"Cost\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c901652",
      "metadata": {
        "id": "1c901652"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- Logistic cost (preferred) ----\n",
        "W2, B2, Z2 = surface(logistic_cost, x_train, y_train, w_range=(-5, 15), b_range=(-10, 10), n=70)\n",
        "plot_surface(W2, B2, Z2, title=\"Logistic Cost vs (w, b)\", zlabel=\"Cost\")\n",
        "\n",
        "# Optional: visualize log of the cost to accentuate the valley shape\n",
        "Z2_log = np.log(np.clip(Z2, 1e-12, None))\n",
        "plot_surface(W2, B2, Z2_log, title=\"log(Logistic Cost) vs (w, b)\", zlabel=\"log(Cost)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7cd61e8",
      "metadata": {
        "id": "e7cd61e8"
      },
      "source": [
        "\n",
        "## Why logistic loss?\n",
        "- With the sigmoid link, squared error can lead to non‑convex landscapes in (w,b) → gradient descent may struggle.\n",
        "- Logistic loss yields a smooth surface suitable for optimization; for linear logistic regression it is convex in (w,b).\n",
        "- The average cost is:\n",
        "  J(w,b) = (1/m) * sum_i loss( f_w,b(x^(i)), y^(i) ),\n",
        "  where loss(f,y) = -[ y*log(f) + (1-y)*log(1-f) ].\n",
        "- In practice you would use gradient descent (or variants) on J(w,b) to learn parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05855551",
      "metadata": {
        "id": "05855551"
      },
      "source": [
        "\n",
        "## Quick executable check\n",
        "\n",
        "The cell below computes J(w,b) on the toy dataset for a few candidate parameters to verify code paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d172eb2f",
      "metadata": {
        "id": "d172eb2f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==========================\n",
        "# 7) Quick executable checks\n",
        "# ==========================\n",
        "candidates = [(0.0, 0.0), (2.0, -4.0), (4.0, -8.0), (6.0, -12.0)]\n",
        "for w, b in candidates:\n",
        "    print(f\"(w={w:>4.1f}, b={b:>5.1f})  squared_error_cost = {squared_error_cost(x_train, y_train, w, b):.5f}\"\n",
        "          f\" | logistic_cost = {logistic_cost(x_train, y_train, w, b):.5f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}