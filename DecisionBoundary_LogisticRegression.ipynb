{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asupraja3/ml-ng-notebooks/blob/main/DecisionBoundary_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "485efa7e",
      "metadata": {
        "id": "485efa7e"
      },
      "source": [
        "\n",
        "# Optional Lab: Logistic Regression — Decision Boundary\n",
        "\n",
        "*Adapted, self‑contained notebook inspired by Andrew Ng's Machine Learning Specialization.*\n",
        "\n",
        "## Goals\n",
        "In this lab you will:\n",
        "- Plot the **decision boundary** for a logistic regression model to better understand what the model is predicting.\n",
        "- See both a **linear** and a **non-linear** decision boundary example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48f13e5a",
      "metadata": {
        "id": "48f13e5a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Setup ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b478e5c0",
      "metadata": {
        "id": "b478e5c0"
      },
      "source": [
        "\n",
        "## Dataset\n",
        "\n",
        "We'll use a tiny 2D dataset with labels \\(y\\in\\{0,1\\}\\). Each row of `X` is \\([x_0, x_1]\\).  \n",
        "Red 'x' will indicate `y=1`, blue circles `y=0` (as in the course).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74c3dd7",
      "metadata": {
        "id": "b74c3dd7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Toy 2D dataset (astype to float for numerical routines) ---\n",
        "X = np.array([\n",
        "    [0.5, 1.5],\n",
        "    [1.0, 1.0],\n",
        "    [1.5, 0.5],\n",
        "    [3.5, 0.5],\n",
        "    [2.0, 2.0],\n",
        "    [1.0, 2.5]\n",
        "]).astype(float)\n",
        "y = np.array([0, 0, 0, 1, 1, 1]).astype(int)\n",
        "\n",
        "print(\"X shape:\", X.shape, \"y shape:\", y.shape, \"dtypes:\", X.dtype, y.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc7faff",
      "metadata": {
        "id": "fdc7faff"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_data(X, y, ax=None, title=None):\n",
        "    \"\"\"Scatter plot of 2D data: circles for y=0, x for y=1.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(5,4))\n",
        "    neg, pos = (y==0), (y==1)\n",
        "    ax.scatter(X[neg,0], X[neg,1], s=70, facecolors='none', edgecolors='tab:blue', label='y=0')\n",
        "    ax.scatter(X[pos,0], X[pos,1], s=70, marker='x', color='tab:red', label='y=1')\n",
        "    ax.set_xlabel(\"$x_0$\")\n",
        "    ax.set_ylabel(\"$x_1$\")\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "    ax.legend()\n",
        "    ax.set_xlim(0,4)\n",
        "    ax.set_ylim(0,3.5)\n",
        "    return ax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70c5ee2d",
      "metadata": {
        "id": "70c5ee2d"
      },
      "source": [
        "\n",
        "## Refresher: Logistic regression & decision boundary\n",
        "\n",
        "A logistic regression model predicts\n",
        "\n",
        "\\[ f_{w,b}(x) = g(w^T x + b),\\quad g(z)=\\frac{1}{1+e^{-z}}. \\]\n",
        "\n",
        "We interpret \\(f_{w,b}(x)\\) as \\(P(y=1\\mid x)\\). With the usual 0.5 threshold:\n",
        "\n",
        "- If \\(f_{w,b}(x) \\ge 0.5\\) (i.e., \\(w^T x + b \\ge 0\\)), predict **1**.  \n",
        "- Else predict **0**.\n",
        "\n",
        "Thus, the **decision boundary** is the set of points where \\(w^T x + b = 0\\). In 2D, this is a **line**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "974dc7cf",
      "metadata": {
        "id": "974dc7cf"
      },
      "source": [
        "\n",
        "### Plotting a given linear boundary\n",
        "\n",
        "Consider the example from the lecture: \\(w=[1,1],\\ b=-3\\).  \n",
        "The boundary equation is \\(x_0 + x_1 - 3 = 0\\) → a line intersecting axes at 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "250bf0a0",
      "metadata": {
        "id": "250bf0a0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Plot the fixed boundary x0 + x1 - 3 = 0 and shade one side ---\n",
        "fig, ax = plt.subplots(figsize=(5,4))\n",
        "plot_data(X, y, ax, title=\"Decision boundary: $x_0 + x_1 - 3 = 0$\")\n",
        "\n",
        "# x1 = 3 - x0\n",
        "x0 = np.linspace(0, 4, 200).astype(float)\n",
        "x1 = 3.0 - x0\n",
        "ax.plot(x0, x1, linewidth=2)\n",
        "\n",
        "# Shade the region where w^T x + b >= 0  -> x0 + x1 - 3 >= 0 -> x1 >= 3 - x0\n",
        "ax.fill_between(x0, x1, 3.5, alpha=0.15)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df255c6",
      "metadata": {
        "id": "6df255c6"
      },
      "source": [
        "\n",
        "## Learned boundary from a fitted model\n",
        "\n",
        "Now let's actually **fit** logistic regression on the dataset and visualize the 0.5 probability contour (i.e., \\(w^T x + b=0\\)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4df3833",
      "metadata": {
        "id": "f4df3833"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Fit logistic regression and plot P=0.5 contour ---\n",
        "clf = LogisticRegression(solver='lbfgs')\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Build a grid and compute probabilities\n",
        "x0_range = np.linspace(0, 4, 300)\n",
        "x1_range = np.linspace(0, 3.5, 300)\n",
        "xx0, xx1 = np.meshgrid(x0_range, x1_range)\n",
        "grid = np.c_[xx0.ravel(), xx1.ravel()].astype(float)\n",
        "probs = clf.predict_proba(grid)[:,1].reshape(xx0.shape)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5,4))\n",
        "plot_data(X, y, ax, title=\"Learned decision boundary (P(y=1|x)=0.5)\")\n",
        "\n",
        "# Draw the 0.5 contour (decision boundary)\n",
        "cs = ax.contour(xx0, xx1, probs, levels=[0.5], linewidths=2)\n",
        "cs.collections[0].set_label('0.5 contour')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Learned w:\", clf.coef_[0], \"b:\", clf.intercept_[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcc5a6a7",
      "metadata": {
        "id": "fcc5a6a7"
      },
      "source": [
        "\n",
        "## Non-linear decision boundary (feature mapping idea)\n",
        "\n",
        "Sometimes a straight line can't separate the classes. If we define\n",
        "\\(z = x_0^2 + x_1^2 - 1\\) and use \\(g(z)\\), the decision boundary \\(z=0\\) is a **circle** of radius 1:\n",
        "\n",
        "\\[ x_0^2 + x_1^2 = 1. \\]\n",
        "\n",
        "Below we just **plot** that boundary to illustrate the idea.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63069747",
      "metadata": {
        "id": "63069747"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Plot a circular boundary: x0^2 + x1^2 = 1 ---\n",
        "theta = np.linspace(0, 2*np.pi, 400).astype(float)\n",
        "circle_x = np.cos(theta) * 1.0 + 0.0\n",
        "circle_y = np.sin(theta) * 1.0 + 0.0\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5,4))\n",
        "plot_data(X, y, ax, title=\"Non-linear boundary example: $x_0^2 + x_1^2 = 1$\")\n",
        "ax.plot(circle_x, circle_y, linewidth=2)\n",
        "# Shade outside region (ŷ=1 side if z >= 0)\n",
        "ax.fill_between(circle_x, circle_y, 3.5, alpha=0.1)\n",
        "plt.show()\n",
        "\n",
        "# Note: To actually *learn* such a boundary with logistic regression,\n",
        "# you can expand features (e.g., [x0, x1, x0^2, x1^2, x0*x1, ...]) and fit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff28f978",
      "metadata": {
        "id": "ff28f978"
      },
      "source": [
        "\n",
        "## Notes / Takeaways\n",
        "\n",
        "- The **decision boundary** for logistic regression with a 0.5 threshold is where \\(w^T x + b = 0\\).  \n",
        "- In 2D, a linear model yields a **line**; with feature mapping (polynomials), you can model **curves** like circles.  \n",
        "- Plotting the boundary helps debug model behavior and understand what regions are classified as positive vs negative.\n",
        "\n",
        "**Try this:**\n",
        "1. Move one or two points in `X` and re-fit; watch how the learned 0.5 contour moves.  \n",
        "2. Add polynomial features and let logistic regression learn a circular/curvy boundary.  \n",
        "3. Change thresholds (0.3/0.7) and see how the decision region changes.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}