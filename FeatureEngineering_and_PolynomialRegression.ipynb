{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asupraja3/ml-ng-notebooks/blob/main/FeatureEngineering_and_PolynomialRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01d11cf7",
      "metadata": {
        "id": "01d11cf7"
      },
      "source": [
        "\n",
        "# Optional Lab: Feature Engineering and Polynomial Regression\n",
        "\n",
        "This notebook demonstrates:\n",
        "- Creating polynomial **features** (e.g., $x, x^2, x^3$)\n",
        "- Fitting models with **gradient descent**\n",
        "- Visualizing fits **with** and **without** feature engineering\n",
        "- Inspecting learned parameters\n",
        "\n",
        "We use a simple target function: $y = 1 + x^2$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd12999b",
      "metadata": {
        "id": "cd12999b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(precision=6, suppress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51cf0c2d",
      "metadata": {
        "id": "51cf0c2d"
      },
      "source": [
        "## Helpers: cost, gradient, and gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4ad287a",
      "metadata": {
        "id": "d4ad287a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict(X, w, b):\n",
        "    return X @ w + b\n",
        "\n",
        "def compute_cost(X, y, w, b):\n",
        "    m = X.shape[0]\n",
        "    e = X @ w + b - y\n",
        "    return (e @ e) / (2*m)\n",
        "\n",
        "def compute_gradient(X, y, w, b):\n",
        "    m = X.shape[0]\n",
        "    e = X @ w + b - y\n",
        "    dj_dw = (X.T @ e) / m\n",
        "    dj_db = np.sum(e) / m\n",
        "    return dj_dw, dj_db\n",
        "\n",
        "def gradient_descent(X, y, w_init=None, b_init=0.0, alpha=1e-2, iters=1000, trace=False):\n",
        "    n = X.shape[1]\n",
        "    w = np.zeros(n) if w_init is None else w_init.astype(float).copy()\n",
        "    b = float(b_init)\n",
        "    J_hist = []\n",
        "    tr = []\n",
        "    for t in range(1, iters+1):\n",
        "        dj_dw, dj_db = compute_gradient(X, y, w, b)\n",
        "        w -= alpha * dj_dw\n",
        "        b -= alpha * dj_db\n",
        "        J = compute_cost(X, y, w, b)\n",
        "        J_hist.append(J)\n",
        "        if trace and t <= 2000:\n",
        "            tr.append((J, w.copy(), b))\n",
        "    return w, b, np.array(J_hist), tr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "624431ef",
      "metadata": {
        "id": "624431ef"
      },
      "source": [
        "## Data: quadratic target $y = 1 + x^2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d03d79",
      "metadata": {
        "id": "a0d03d79"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create target data\n",
        "x = np.arange(0, 20, 1, dtype=float)\n",
        "y = 1 + x**2\n",
        "x_lin = x.reshape(-1, 1)  # column vector for linear feature\n",
        "\n",
        "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\")\n",
        "plt.title(\"Target data: y = 1 + x^2\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6722ea04",
      "metadata": {
        "id": "6722ea04"
      },
      "source": [
        "## 1) No feature engineering (linear model on raw x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9532f2c9",
      "metadata": {
        "id": "9532f2c9"
      },
      "outputs": [],
      "source": [
        "\n",
        "X = x_lin  # only x\n",
        "w, b, J_hist, _ = gradient_descent(X, y, alpha=1e-2, iters=1000)\n",
        "\n",
        "print(\"No FE → w:\", w, \"b:\", b, \" final cost:\", J_hist[-1])\n",
        "\n",
        "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\")\n",
        "plt.plot(x, X @ w + b, label=\"Predicted Value\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.title(\"No feature engineering\"); plt.legend(); plt.show()\n",
        "\n",
        "for t in range(0, 1001, 100):\n",
        "    if t == 0:\n",
        "        print(f\"Iteration {t:4d}, Cost: {J_hist[0]:.6e}\")\n",
        "    else:\n",
        "        print(f\"Iteration {t:4d}, Cost: {J_hist[t-1]:.6e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f014202",
      "metadata": {
        "id": "3f014202"
      },
      "source": [
        "## 2) Polynomial feature: replace x with $x^2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16ba18b6",
      "metadata": {
        "id": "16ba18b6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Engineer feature: x^2 instead of x\n",
        "x_sq = (x**2).reshape(-1, 1)\n",
        "X = x_sq\n",
        "\n",
        "w2, b2, J_hist2, _ = gradient_descent(X, y, alpha=1e-5, iters=10_000)\n",
        "\n",
        "print(\"Using x^2 → w:\", w2, \"b:\", b2, \" final cost:\", J_hist2[-1])\n",
        "\n",
        "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\")\n",
        "plt.plot(x, X @ w2 + b2, label=\"Predicted Value\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.title(\"Added x^2 feature\"); plt.legend(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f15f73b9",
      "metadata": {
        "id": "f15f73b9"
      },
      "source": [
        "## 3) Polynomial regression with multiple features: $[x, x^2, x^3]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "270c00fa",
      "metadata": {
        "id": "270c00fa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Engineer multiple polynomial features\n",
        "X = np.c_[x_lin, x**2, x**3]  # shape (m, 3)\n",
        "\n",
        "w3, b3, J_hist3, _ = gradient_descent(X, y, alpha=1e-7, iters=10_000)\n",
        "\n",
        "print(\"Using [x, x^2, x^3] → w:\", w3, \"b:\", b3, \" final cost:\", J_hist3[-1])\n",
        "\n",
        "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\")\n",
        "plt.plot(x, X @ w3 + b3, label=\"Predicted Value\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.title(\"x, x^2, x^3 features\"); plt.legend(); plt.show()\n",
        "\n",
        "# Display polynomial fit explicitly\n",
        "print(\"\\nModel (approx): y ≈ \"\n",
        "      f\"{w3[0]:.4f}·x + {w3[1]:.4f}·x^2 + {w3[2]:.4e}·x^3 + {b3:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd8cbf69",
      "metadata": {
        "id": "cd8cbf69"
      },
      "source": [
        "## 4) Cost curves for each setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b75c9ebc",
      "metadata": {
        "id": "b75c9ebc"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(J_hist, label=\"No FE (linear x)\")\n",
        "plt.plot(J_hist2, label=\"x^2 only\")\n",
        "plt.plot(J_hist3, label=\"[x, x^2, x^3]\")\n",
        "plt.xlabel(\"iteration\"); plt.ylabel(\"cost\"); plt.title(\"Cost vs Iteration\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}