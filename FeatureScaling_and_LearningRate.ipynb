{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asupraja3/ml-ng-notebooks/blob/main/FeatureScaling_and_LearningRate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "367ff930",
      "metadata": {
        "id": "367ff930"
      },
      "source": [
        "\n",
        "# Optional Lab: Feature Scaling and Learning Rate (Multi‑variable)\n",
        "\n",
        "This self‑contained notebook shows:\n",
        "- How to **scale features** (min‑max, mean normalization, **z‑score**)\n",
        "- How scaling affects **gradient descent convergence**\n",
        "- How to choose/tune the **learning rate (α)** with visual diagnostics\n",
        "\n",
        "Libraries: `numpy`, `matplotlib` (no external datasets required).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25578c76",
      "metadata": {
        "id": "25578c76"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89b63a38",
      "metadata": {
        "id": "89b63a38"
      },
      "source": [
        "## 1) Create a synthetic multi‑feature dataset (house pricing style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94205f90",
      "metadata": {
        "id": "94205f90"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Features: [size(sqft), bedrooms, age(years), floors]\n",
        "rng = np.random.default_rng(0)\n",
        "m = 300\n",
        "\n",
        "size = rng.uniform(600, 3500, m)         # large numeric range\n",
        "beds = rng.integers(1, 6, m)             # small integer range\n",
        "age  = rng.uniform(0, 40, m)             # medium range\n",
        "floors = rng.integers(1, 4, m)           # small range\n",
        "\n",
        "X_train = np.c_[size, beds, age, floors].astype(float)\n",
        "\n",
        "# Ground-truth linear model (unknown to learner)\n",
        "true_w = np.array([220.0, 30.0, -2.0, 15.0])\n",
        "true_b = 50_000.0\n",
        "\n",
        "noise = rng.normal(0, 25_000, m)\n",
        "y_train = X_train @ true_w + true_b + noise\n",
        "\n",
        "X_features = [\"size(sqft)\", \"bedrooms\", \"age\", \"floors\"]\n",
        "X_train[:3], y_train[:3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "262c8c5b",
      "metadata": {
        "id": "262c8c5b"
      },
      "source": [
        "## 2) Visualize feature distributions (before/after scaling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32e1b6e5",
      "metadata": {
        "id": "32e1b6e5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def norm_plot(ax, x, bins=30):\n",
        "    \"\"\"Plot histogram and overlay a normal pdf with same mean/std.\"\"\"\n",
        "    mu, sigma = np.mean(x), np.std(x)\n",
        "    ax.hist(x, bins=bins, alpha=0.6, density=True)\n",
        "    # overlay normal curve\n",
        "    xs = np.linspace(mu - 4*sigma, mu + 4*sigma, 200)\n",
        "    if sigma > 1e-12:\n",
        "        pdf = (1.0/(sigma*np.sqrt(2*np.pi))) * np.exp(-0.5*((xs-mu)/sigma)**2)\n",
        "        ax.plot(xs, pdf, linewidth=2)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    return mu, sigma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f1641a3",
      "metadata": {
        "id": "8f1641a3"
      },
      "source": [
        "## 3) Scaling utilities (min‑max, mean normalization, z‑score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed6aa81",
      "metadata": {
        "id": "6ed6aa81"
      },
      "outputs": [],
      "source": [
        "\n",
        "def minmax_scale(X):\n",
        "    X = X.astype(float)\n",
        "    X_min = X.min(axis=0)\n",
        "    X_max = X.max(axis=0)\n",
        "    # avoid division by zero\n",
        "    denom = np.where(X_max - X_min == 0, 1.0, X_max - X_min)\n",
        "    X_s = (X - X_min) / denom\n",
        "    return X_s, X_min, X_max\n",
        "\n",
        "def mean_normalize(X):\n",
        "    X = X.astype(float)\n",
        "    mu = X.mean(axis=0)\n",
        "    X_min = X.min(axis=0)\n",
        "    X_max = X.max(axis=0)\n",
        "    denom = np.where(X_max - X_min == 0, 1.0, X_max - X_min)\n",
        "    X_s = (X - mu) / denom\n",
        "    return X_s, mu, (X_min, X_max)\n",
        "\n",
        "def zscore_normalize(X):\n",
        "    X = X.astype(float)\n",
        "    mu = X.mean(axis=0)\n",
        "    sigma = X.std(axis=0, ddof=0)\n",
        "    sigma = np.where(sigma == 0, 1.0, sigma)\n",
        "    X_s = (X - mu) / sigma\n",
        "    return X_s, mu, sigma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "408d9886",
      "metadata": {
        "id": "408d9886"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot distributions *before* normalization\n",
        "fig, ax = plt.subplots(1, X_train.shape[1], figsize=(14, 3))\n",
        "for i in range(X_train.shape[1]):\n",
        "    norm_plot(ax[i], X_train[:, i])\n",
        "    ax[i].set_xlabel(X_features[i])\n",
        "ax[0].set_ylabel(\"density\")\n",
        "fig.suptitle(\"Distributions BEFORE normalization\")\n",
        "plt.show()\n",
        "\n",
        "# Z-score normalize\n",
        "X_norm, mu, sigma = zscore_normalize(X_train)\n",
        "\n",
        "# Plot distributions *after* normalization\n",
        "fig, ax = plt.subplots(1, X_norm.shape[1], figsize=(14, 3))\n",
        "for i in range(X_norm.shape[1]):\n",
        "    norm_plot(ax[i], X_norm[:, i])\n",
        "    ax[i].set_xlabel(X_features[i])\n",
        "ax[0].set_ylabel(\"density\")\n",
        "fig.suptitle(\"Distributions AFTER z-score normalization\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e23219",
      "metadata": {
        "id": "37e23219"
      },
      "source": [
        "## 4) Linear regression helpers (cost, gradient, GD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "945776ed",
      "metadata": {
        "id": "945776ed"
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict(X, w, b):\n",
        "    return X @ w + b\n",
        "\n",
        "def compute_cost(X, y, w, b):\n",
        "    m = X.shape[0]\n",
        "    errors = X @ w + b - y\n",
        "    return (errors @ errors) / (2*m)\n",
        "\n",
        "def compute_gradient(X, y, w, b):\n",
        "    m = X.shape[0]\n",
        "    errors = X @ w + b - y                 # (m,)\n",
        "    dj_dw = (X.T @ errors) / m             # (n,)\n",
        "    dj_db = np.sum(errors) / m             # scalar\n",
        "    return dj_dw, dj_db\n",
        "\n",
        "def gradient_descent(X, y, w_init, b_init, alpha, num_iters):\n",
        "    w = w_init.copy().astype(float)\n",
        "    b = float(b_init)\n",
        "    J_hist = []\n",
        "    trace = []   # store (cost, w.copy(), b) for plotting diagnostics\n",
        "    for t in range(1, num_iters+1):\n",
        "        dj_dw, dj_db = compute_gradient(X, y, w, b)\n",
        "        w -= alpha * dj_dw\n",
        "        b -= alpha * dj_db\n",
        "        J = compute_cost(X, y, w, b)\n",
        "        J_hist.append(J)\n",
        "        if t <= 1000:  # limit stored history to keep light\n",
        "            trace.append((J, w.copy(), b))\n",
        "    return w, b, np.array(J_hist), trace\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52ece207",
      "metadata": {
        "id": "52ece207"
      },
      "source": [
        "## 5) Train **without** scaling (expect slow/unstable with same α)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab18d523",
      "metadata": {
        "id": "ab18d523"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Start with small random parameters\n",
        "n = X_train.shape[1]\n",
        "w0 = np.zeros(n)\n",
        "b0 = 0.0\n",
        "\n",
        "alpha = 1e-7        # tiny step to avoid divergence on unscaled data\n",
        "iters = 200\n",
        "\n",
        "w_ns, b_ns, J_ns, tr_ns = gradient_descent(X_train, y_train, w0, b0, alpha, iters)\n",
        "\n",
        "print(\"Unscaled   → final cost:\", J_ns[-1])\n",
        "print(\"Unscaled   → w:\", w_ns, \"b:\", round(b_ns, 2))\n",
        "\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(J_ns)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.title(\"Cost vs iteration (UNSCALED)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb21cc9e",
      "metadata": {
        "id": "eb21cc9e"
      },
      "source": [
        "## 6) Train **with z‑score scaling** (can use a much larger α)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f323d230",
      "metadata": {
        "id": "f323d230"
      },
      "outputs": [],
      "source": [
        "\n",
        "alpha_s = 5e-2      # much larger step on scaled features\n",
        "iters_s = 200\n",
        "\n",
        "w_s, b_s, J_s, tr_s = gradient_descent(X_norm, y_train, w0, b0, alpha_s, iters_s)\n",
        "\n",
        "print(\"Scaled (z) → final cost:\", J_s[-1])\n",
        "print(\"Scaled (z) → w:\", w_s, \"b:\", round(b_s, 2))\n",
        "\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(J_s)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.title(\"Cost vs iteration (SCALED, z‑score)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2fa20b7",
      "metadata": {
        "id": "d2fa20b7"
      },
      "source": [
        "## 7) Targets vs predictions (using **normalized** model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce74dcc",
      "metadata": {
        "id": "7ce74dcc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predictions with normalized features\n",
        "yp = predict(X_norm, w_s, b_s)\n",
        "\n",
        "fig, ax = plt.subplots(1, X_norm.shape[1], figsize=(14,3), sharey=True)\n",
        "for i in range(X_norm.shape[1]):\n",
        "    ax[i].scatter(X_train[:, i], y_train, label=\"target\")\n",
        "    ax[i].scatter(X_train[:, i], yp, marker='x', label=\"predict\")\n",
        "    ax[i].set_xlabel(X_features[i])\n",
        "ax[0].set_ylabel(\"Price\")\n",
        "ax[0].legend()\n",
        "fig.suptitle(\"Target vs Prediction using z‑score normalized model\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eb472eb",
      "metadata": {
        "id": "4eb472eb"
      },
      "source": [
        "## 8) Learning rate sweeps & parameter trace diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c6a0ad3",
      "metadata": {
        "id": "8c6a0ad3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_cost_and_param_trace(trace, param_index=0, title_suffix=\"\"):\n",
        "    \"\"\"Plot (left) cost vs iter and (right) a chosen weight evolution.\"\"\"\n",
        "    J = [t[0] for t in trace]\n",
        "    W = [t[1] for t in trace]\n",
        "    B = [t[2] for t in trace]\n",
        "    w_series = [w[param_index] for w in W]\n",
        "\n",
        "    fig = plt.figure(figsize=(10,3))\n",
        "    ax1 = plt.subplot(1,2,1)\n",
        "    ax1.plot(J)\n",
        "    ax1.set_xlabel(\"iteration\")\n",
        "    ax1.set_ylabel(\"cost\")\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    ax2 = plt.subplot(1,2,2)\n",
        "    ax2.plot(w_series)\n",
        "    ax2.set_xlabel(\"iteration\")\n",
        "    ax2.set_ylabel(f\"w[{param_index}]\")\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    fig.suptitle(f\"Convergence diagnostics{title_suffix}\")\n",
        "    plt.show()\n",
        "\n",
        "alphas = [0.5, 0.1, 0.05, 0.01, 0.005]\n",
        "iters = 120\n",
        "\n",
        "for a in alphas:\n",
        "    _, _, _, tr = gradient_descent(X_norm, y_train, np.zeros(X_norm.shape[1]), 0.0, a, iters)\n",
        "    plot_cost_and_param_trace(tr, 0, title_suffix=f\"  (alpha={a})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b700f36a",
      "metadata": {
        "id": "b700f36a"
      },
      "source": [
        "## 9) Optional: Convert normalized weights back to original feature scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba75471c",
      "metadata": {
        "id": "ba75471c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# If the model was trained on z‑score normalized features:\n",
        "#   y ≈ ( (x - mu)/sigma )·w_s + b_s\n",
        "# Expand to get: y ≈ x·(w_s / sigma) + (b_s - mu·(w_s / sigma))\n",
        "w_back = w_s / sigma\n",
        "b_back = b_s - (mu @ w_back)\n",
        "\n",
        "print(\"Recovered weights in ORIGINAL scale:\")\n",
        "print(\"w_back:\", w_back)\n",
        "print(\"b_back:\", round(b_back, 2))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}