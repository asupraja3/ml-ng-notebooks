{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asupraja3/ml-ng-notebooks/blob/main/GradientDescent_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "877f12d1",
      "metadata": {
        "id": "877f12d1"
      },
      "source": [
        "\n",
        "# Optional Lab: Gradient Descent for Logistic Regression\n",
        "\n",
        "**Goals**\n",
        "- Explore the gradient descent update code for logistic regression\n",
        "- Visualize the cost decreasing over iterations\n",
        "- Build intuition for the parameters \\(w, b\\) using a tiny 2‑feature toy dataset\n",
        "\n",
        "**What you should know after this lab**\n",
        "1. How the sigmoid function maps $z = w^{\\top} x + b$ to probabilities\n",
        "2. How to compute the logistic loss for a batch of examples\n",
        "3. How to compute gradients $\\frac{\\partial J}{\\partial w_j}$ and $\\frac{\\partial J}{\\partial b}$ in loops and vectorized form\n",
        "4. How to implement batch gradient descent and diagnose convergence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "102e8ff4",
      "metadata": {
        "id": "102e8ff4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==== Imports ====\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# (No seaborn. One chart per figure.)\n",
        "plt.rcParams[\"figure.figsize\"] = (5, 4)\n",
        "plt.rcParams[\"font.size\"] = 12\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27b1b1b",
      "metadata": {
        "id": "e27b1b1b"
      },
      "source": [
        "\n",
        "## Data set\n",
        "\n",
        "We'll reuse a small, two‑feature dataset in the spirit of the course's labs.\n",
        "- Each example \\(x^{(i)} = [x_0^{(i)}, x_1^{(i)}]\\)\n",
        "- Labels \\(y^{(i)} \\in \\{0,1\\}\\)\n",
        "\n",
        "This is intentionally small so you can step through the math and the plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c5add8f",
      "metadata": {
        "id": "8c5add8f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Two features (x0, x1). Each row is one example.\n",
        "X_train = np.array([\n",
        "    [0.5, 1.5],\n",
        "    [1.0, 1.0],\n",
        "    [1.5, 0.5],\n",
        "    [3.0, 0.5],\n",
        "    [2.0, 2.0],\n",
        "    [1.0, 2.5]\n",
        "], dtype=float)\n",
        "\n",
        "# Corresponding labels\n",
        "y_train = np.array([0, 0, 0, 1, 1, 1], dtype=float)\n",
        "\n",
        "m, n = X_train.shape\n",
        "(m, n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53eb0fa5",
      "metadata": {
        "id": "53eb0fa5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_data(X, y):\n",
        "    \"\"\"Scatter-plot the tiny dataset.\n",
        "\"\n",
        "    \"We avoid setting explicit colors to follow the constraints.\n",
        "\"\n",
        "    \"\"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    pos = y == 1\n",
        "    neg = y == 0\n",
        "    ax.plot(X[pos, 0], X[pos, 1], 'x', label=\"y=1\")\n",
        "    ax.plot(X[neg, 0], X[neg, 1], 'o', label=\"y=0\")\n",
        "    ax.set_xlabel(\"$x_0$\")\n",
        "    ax.set_ylabel(\"$x_1$\")\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_data(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1579731c",
      "metadata": {
        "id": "1579731c"
      },
      "source": [
        "\n",
        "## Logistic model and sigmoid\n",
        "\n",
        "We use:\n",
        "\\[\n",
        "z^{(i)} = w^\\top x^{(i)} + b,\\qquad \\hat y^{(i)} = \\sigma(z^{(i)}) = \\frac{1}{1 + e^{-z^{(i)}}}\n",
        "\\]\n",
        "\n",
        "- \\( \\hat y^{(i)} \\) is interpreted as \\(P(y=1 \\mid x^{(i)})\\).\n",
        "- The decision boundary at probability 0.5 corresponds to \\( w^\\top x + b = 0 \\).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf0be9a3",
      "metadata": {
        "id": "bf0be9a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"Numerically stable sigmoid.\"\"\"\n",
        "    z = np.clip(z, -500, 500)\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def predict_proba(X, w, b):\n",
        "    \"\"\"Return probabilities for class 1 for each example in X.\"\"\"\n",
        "    return sigmoid(X @ w + b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19465459",
      "metadata": {
        "id": "19465459"
      },
      "source": [
        "\n",
        "## Logistic loss (cost)\n",
        "\n",
        "For a batch of \\(m\\) examples:\n",
        "\\[\n",
        "J(w,b) = -\\frac{1}{m}\\sum_{i=1}^m \\Big( y^{(i)}\\log \\hat y^{(i)} + (1-y^{(i)})\\log(1-\\hat y^{(i)}) \\Big)\n",
        "\\]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db08dfaf",
      "metadata": {
        "id": "db08dfaf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_cost_logistic(X, y, w, b):\n",
        "    \"\"\"Compute the average logistic loss J(w,b) over the batch.\"\"\"\n",
        "    m = X.shape[0]\n",
        "    z = X @ w + b\n",
        "    yhat = sigmoid(z)\n",
        "    eps = 1e-12\n",
        "    loss = -(y * np.log(yhat + eps) + (1 - y) * np.log(1 - yhat + eps))\n",
        "    return np.sum(loss) / m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb12094",
      "metadata": {
        "id": "8cb12094"
      },
      "source": [
        "\n",
        "## Gradients — loop version (pedagogical)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26bbea1c",
      "metadata": {
        "id": "26bbea1c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_gradient_logistic_loops(X, y, w, b):\n",
        "    \"\"\"Compute dJ/dw and dJ/db using explicit loops (readable).\"\"\"\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(n, dtype=float)\n",
        "    dj_db = 0.0\n",
        "    for i in range(m):\n",
        "        z_i = np.dot(X[i], w) + b\n",
        "        yhat_i = sigmoid(z_i)\n",
        "        err_i = yhat_i - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] += err_i * X[i, j]\n",
        "        dj_db += err_i\n",
        "    dj_dw /= m\n",
        "    dj_db /= m\n",
        "    return dj_dw, dj_db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04c6a35",
      "metadata": {
        "id": "a04c6a35"
      },
      "source": [
        "\n",
        "## Gradients — vectorized (fast & concise)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c0526aa",
      "metadata": {
        "id": "0c0526aa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_gradient_logistic_vectorized(X, y, w, b):\n",
        "    \"\"\"Vectorized gradient for logistic regression.\"\"\"\n",
        "    m = X.shape[0]\n",
        "    yhat = predict_proba(X, w, b)\n",
        "    err = yhat - y\n",
        "    dj_dw = (X.T @ err) / m\n",
        "    dj_db = np.sum(err) / m\n",
        "    return dj_dw, dj_db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b8a3266",
      "metadata": {
        "id": "4b8a3266"
      },
      "source": [
        "\n",
        "## Batch Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04a442e",
      "metadata": {
        "id": "a04a442e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gradient_descent_logistic(X, y, w_in, b_in, alpha, num_iters, use_vectorized=True):\n",
        "    \"\"\"Perform batch gradient descent for logistic regression.\"\"\"\n",
        "    w = w_in.copy().astype(float)\n",
        "    b = float(b_in)\n",
        "    J_history, trace = [], []\n",
        "    for it in range(num_iters):\n",
        "        if use_vectorized:\n",
        "            dj_dw, dj_db = compute_gradient_logistic_vectorized(X, y, w, b)\n",
        "        else:\n",
        "            dj_dw, dj_db = compute_gradient_logistic_loops(X, y, w, b)\n",
        "        w -= alpha * dj_dw\n",
        "        b -= alpha * dj_db\n",
        "        J = compute_cost_logistic(X, y, w, b)\n",
        "        J_history.append(J)\n",
        "        trace.append((J, w.copy(), b))\n",
        "        if num_iters <= 50 and (it % max(1, num_iters // 10) == 0):\n",
        "            print(f\"iter {it:4d}  J={J:.6f}  ||grad||={np.linalg.norm(dj_dw):.4e}  db={dj_db:.4e}\")\n",
        "    return w, b, J_history, trace\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87c15ca6",
      "metadata": {
        "id": "87c15ca6"
      },
      "source": [
        "\n",
        "## Run it on the toy data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88cd9ac9",
      "metadata": {
        "id": "88cd9ac9"
      },
      "outputs": [],
      "source": [
        "\n",
        "w0 = np.zeros(X_train.shape[1], dtype=float)\n",
        "b0 = 0.0\n",
        "alpha = 0.1\n",
        "num_iters = 10000\n",
        "\n",
        "w_out, b_out, J_hist, trace = gradient_descent_logistic(\n",
        "    X_train, y_train, w0, b0, alpha, num_iters, use_vectorized=True\n",
        ")\n",
        "print(\"Learned parameters:\")\n",
        "print(\"w:\", w_out)\n",
        "print(\"b:\", b_out)\n",
        "print(\"Final cost:\", J_hist[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b19c9079",
      "metadata": {
        "id": "b19c9079"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot log(cost) vs iteration to inspect convergence\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(np.log(np.maximum(J_hist, 1e-20)))\n",
        "ax.set_xlabel(\"Iteration\")\n",
        "ax.set_ylabel(\"log(Cost)\")\n",
        "ax.set_title(\"Convergence (log cost)\")\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b3f42a",
      "metadata": {
        "id": "02b3f42a"
      },
      "source": [
        "\n",
        "## Visualize probabilities and the decision boundary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "225a4a2e",
      "metadata": {
        "id": "225a4a2e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict_proba_grid(w, b, x0_min, x0_max, x1_min, x1_max, step):\n",
        "    xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, step),\n",
        "                           np.arange(x1_min, x1_max, step))\n",
        "    grid = np.c_[xx0.ravel(), xx1.ravel()]\n",
        "    probs = predict_proba(grid, w, b).reshape(xx0.shape)\n",
        "    return xx0, xx1, probs\n",
        "\n",
        "def plot_decision_boundary(X, y, w, b, grid_step=0.05, padding=0.5):\n",
        "    x0_min, x0_max = X[:,0].min()-padding, X[:,0].max()+padding\n",
        "    x1_min, x1_max = X[:,1].min()-padding, X[:,1].max()+padding\n",
        "    xx0, xx1, probs = predict_proba_grid(w, b, x0_min, x0_max, x1_min, x1_max, grid_step)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    cs = ax.contourf(xx0, xx1, probs, levels=20, alpha=0.6)\n",
        "    cbar = fig.colorbar(cs); cbar.set_label(\"P(y=1|x)\")\n",
        "    pos = y == 1; neg = y == 0\n",
        "    ax.plot(X[pos, 0], X[pos, 1], 'x', label=\"y=1\")\n",
        "    ax.plot(X[neg, 0], X[neg, 1], 'o', label=\"y=0\")\n",
        "    ax.contour(xx0, xx1, probs, levels=[0.5], linewidths=2)\n",
        "    ax.set_xlabel(\"$x_0$\"); ax.set_ylabel(\"$x_1$\")\n",
        "    ax.set_title(\"Probability field & decision boundary (0.5)\")\n",
        "    ax.legend(loc=\"best\"); ax.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(X_train, y_train, w_out, b_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ad44122",
      "metadata": {
        "id": "4ad44122"
      },
      "source": [
        "\n",
        "## (Optional) Cost as a function of \\(w_0\\) and \\(b\\)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "906a760a",
      "metadata": {
        "id": "906a760a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_cost_landscape_w0_b(X, y, w, b, w0_range=(-1, 7), b_range=(-14, 14), steps=120):\n",
        "    w = w.copy()\n",
        "    w1_fixed = w[1]\n",
        "    w0_vals = np.linspace(w0_range[0], w0_range[1], steps)\n",
        "    b_vals  = np.linspace(b_range[0],  b_range[1],  steps)\n",
        "    W0, B = np.meshgrid(w0_vals, b_vals)\n",
        "    J = np.zeros_like(W0)\n",
        "    for i in range(steps):\n",
        "        for j in range(steps):\n",
        "            w[0] = W0[i, j]; btmp = B[i, j]\n",
        "            J[i, j] = compute_cost_logistic(X, y, w, btmp)\n",
        "    fig, ax = plt.subplots()\n",
        "    cs = ax.contour(W0, B, np.log(np.maximum(J, 1e-20)), levels=25)\n",
        "    ax.set_xlabel(\"w0\"); ax.set_ylabel(\"b\")\n",
        "    ax.set_title(\"log(Cost(w0, b)) with w1 fixed\")\n",
        "    plt.clabel(cs, inline=True, fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_cost_landscape_w0_b(X_train, y_train, w_out, b_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a0738fe",
      "metadata": {
        "id": "9a0738fe"
      },
      "source": [
        "\n",
        "## Takeaways\n",
        "\n",
        "- Logistic regression models the log-odds linearly in the features; sigmoid maps to probability.\n",
        "- Batch gradient descent updates \\(w,b\\) using the average gradient.\n",
        "- Monitoring **log(cost)** helps diagnose convergence.\n",
        "- The decision boundary is where \\(w^\\top x + b = 0\\).\n",
        "- Prefer vectorized code in practice; loops are great for learning.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}