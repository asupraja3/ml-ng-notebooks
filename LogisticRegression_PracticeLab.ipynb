{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asupraja3/ml-ng-notebooks/blob/main/LogisticRegression_PracticeLab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3d12a50",
      "metadata": {
        "id": "b3d12a50"
      },
      "source": [
        "\n",
        "# Practice Lab: Logistic Regression\n",
        "\n",
        "This lab will help you **practice logistic regression** concepts from Andrew Ng's Machine Learning course.\n",
        "\n",
        "### You will practice:\n",
        "1. Sigmoid function implementation and visualization  \n",
        "2. Logistic regression hypothesis and cost function  \n",
        "3. Gradient descent updates  \n",
        "4. Decision boundary visualization  \n",
        "5. Model evaluation (accuracy)  \n",
        "6. Feature scaling and vectorization  \n",
        "7. Regularization (overview + implementation exercise)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89224959",
      "metadata": {
        "id": "89224959"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "75722226",
      "metadata": {
        "id": "75722226"
      },
      "source": [
        "\n",
        "## 1. Sigmoid Function\n",
        "\n",
        "The logistic regression hypothesis uses the **sigmoid function**:  \n",
        "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
        "\n",
        "âœ… **Exercise:** Implement `sigmoid(z)` and test on values [-10, 0, 10].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f50dfa92",
      "metadata": {
        "id": "f50dfa92"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c26f3585",
      "metadata": {
        "id": "c26f3585"
      },
      "source": [
        "\n",
        "## 2. Logistic Regression Hypothesis & Cost\n",
        "\n",
        "**Hypothesis:**  \n",
        "$ h(x) = \\sigma(w^T x + b) $\n",
        "\n",
        "**Loss for one example:**  \n",
        "$ L = - \\big( y \\log(h(x)) + (1-y)\\log(1-h(x)) \\big) $\n",
        "\n",
        "**Cost function:**  \n",
        "$ J(w,b) = -\\frac{1}{m} \\sum_{i=1}^m \\Big[ y^{(i)} \\log(h(x^{(i)})) + (1-y^{(i)}) \\log(1-h(x^{(i)})) \\Big] $\n",
        "\n",
        "âœ… **Exercise:** Implement the cost function `compute_cost_logistic(X, y, w, b)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef55d6f0",
      "metadata": {
        "id": "ef55d6f0"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "728f5053",
      "metadata": {
        "id": "728f5053"
      },
      "source": [
        "\n",
        "## 3. Gradient Descent for Logistic Regression\n",
        "\n",
        "**Gradients:**\n",
        "\n",
        "$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m \\big( h(x^{(i)}) - y^{(i)} \\big) x^{(i)} $\n",
        "\n",
        "$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m \\big( h(x^{(i)}) - y^{(i)} \\big) $\n",
        "\n",
        "\n",
        "âœ… **Exercise:** Implement `compute_gradient_logistic` and `gradient_descent_logistic`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c96598d4",
      "metadata": {
        "id": "c96598d4"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6056eb0c",
      "metadata": {
        "id": "6056eb0c"
      },
      "source": [
        "\n",
        "## 4. Decision Boundary Visualization\n",
        "\n",
        "For 2D data, the decision boundary is defined by:  \n",
        "\n",
        "$ w^T x + b = 0 $  \n",
        "\n",
        "Expanding for two features $x_1$ and $x_2$:  \n",
        "\n",
        "$ w_1 x_1 + w_2 x_2 + b = 0 $  \n",
        "\n",
        "We can solve for $x_2$:  \n",
        "\n",
        "$ x_2 = -\\frac{w_1}{w_2} x_1 - \\frac{b}{w_2} $\n",
        "\n",
        "\n",
        "âœ… **Exercise:** Train logistic regression on synthetic data and plot boundary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3787266b",
      "metadata": {
        "id": "3787266b"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baa7a518",
      "metadata": {
        "id": "baa7a518"
      },
      "source": [
        "\n",
        "## 5. Model Evaluation\n",
        "\n",
        "$ \\text{Accuracy} = \\dfrac{\\text{Number of correct predictions}}{\\text{Total number of predictions}} $\n",
        "\n",
        "\n",
        "âœ… **Exercise:** Implement accuracy function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d621ae17",
      "metadata": {
        "id": "d621ae17"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be6c0475",
      "metadata": {
        "id": "be6c0475"
      },
      "source": [
        "\n",
        "## 6. Feature Scaling & Vectorization\n",
        "\n",
        "- Scaling helps convergence.  \n",
        "- Vectorization speeds up training.\n",
        "\n",
        "âœ… **Exercise:** Scale X and retrain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "744b9f3a",
      "metadata": {
        "id": "744b9f3a"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c46adef9",
      "metadata": {
        "id": "c46adef9"
      },
      "source": [
        "\n",
        "## 7. Regularization (Bonus)\n",
        "\n",
        "**Regularized Cost Function:**  \n",
        "\n",
        "$ J(w,b) = \\text{Logistic Loss} + \\dfrac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2 $  \n",
        "\n",
        "where the logistic loss is:  \n",
        "\n",
        "$ -\\dfrac{1}{m} \\sum_{i=1}^m \\Big[ y^{(i)} \\log(h(x^{(i)})) + (1-y^{(i)}) \\log(1-h(x^{(i)})) \\Big] $\n",
        "\n",
        "\n",
        "âœ… **Exercise:** Modify cost and gradient functions to include L2 penalty.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fa5f270",
      "metadata": {
        "id": "8fa5f270"
      },
      "source": [
        "\n",
        "## âœ… Takeaways\n",
        "\n",
        "- Logistic regression outputs **probabilities** using sigmoid.  \n",
        "- Cost function: **log-loss**, convex and well-suited for classification.  \n",
        "- Gradient descent optimizes weights iteratively.  \n",
        "- Decision boundary is linear in features.  \n",
        "- Feature mapping + regularization improve performance.  \n",
        "- Scaling accelerates convergence.\n",
        "\n",
        "Now, experiment with different datasets, learning rates, and Î» values! ðŸš€\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}