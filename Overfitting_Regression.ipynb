{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asupraja3/ml-ng-notebooks/blob/main/Overfitting_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf55dbb",
      "metadata": {
        "id": "5cf55dbb"
      },
      "source": [
        "# Optional Lab (Ungraded): Overfitting — Colab Notebook\n",
        "\n",
        "**Goals**  \n",
        "In this lab, you will:\n",
        "- See how **model complexity** (e.g., polynomial degree) affects **bias/variance**.  \n",
        "- Explore the impact of **noise** and **outliers** on overfitting.  \n",
        "- Use **regularization** (L2) to mitigate overfitting.  \n",
        "- Compare performance on **train vs. test** sets.  \n",
        "- Try both **regression** and **classification** examples.\n",
        "\n",
        "> This re-creation mirrors the spirit of Andrew Ng's ML \"Overfitting\" optional lab. It provides interactive controls and detailed comments so you can deeply understand what's happening."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b85d98b1",
      "metadata": {
        "id": "b85d98b1"
      },
      "outputs": [],
      "source": [
        "#@title Setup (installs and imports)\n",
        "%pip -q install ipywidgets scikit-learn --upgrade\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, interactive_output, IntSlider, FloatSlider, Dropdown, Checkbox, VBox, HBox, Button\n",
        "from IPython.display import display, clear_output\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, log_loss\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.rcParams.update({\"figure.figsize\": (7, 4.5), \"axes.grid\": True, \"grid.alpha\": 0.3})\n",
        "rng = np.random.default_rng(42)\n",
        "print(\"Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbaa3267",
      "metadata": {
        "id": "bbaa3267"
      },
      "source": [
        "## What is Overfitting?\n",
        "\n",
        "Overfitting happens when a model learns **noise** or **idiosyncrasies** in the training set rather than the underlying pattern.  \n",
        "Symptoms:\n",
        "- Very **low training error** but **high test error**.\n",
        "- Model appears **too wiggly/complex** relative to the amount and quality of data.\n",
        "\n",
        "**Causes**\n",
        "- High model complexity (e.g., high polynomial degree, very deep trees).\n",
        "- Small datasets or high noise.\n",
        "- Outliers that pull the model in weird directions.\n",
        "\n",
        "**Solutions**\n",
        "- Get **more data** (reduces variance).\n",
        "- **Reduce model complexity** (e.g., lower degree).\n",
        "- Apply **regularization** (e.g., L2).\n",
        "- **Feature selection** / remove noisy features.\n",
        "- Use **cross-validation** and **early stopping**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22754926",
      "metadata": {
        "id": "22754926"
      },
      "outputs": [],
      "source": [
        "#@title Helpers: Data generators for regression and classification\n",
        "def make_regression_data(n=30, noise=10.0, x_min=0.0, x_max=30.0):\n",
        "    \"\"\"\n",
        "    Create 1-D regression data from a (hidden) quadratic with noise.\n",
        "    y_true = 0.5*x^2 - 3x + 10\n",
        "    Returns: x (n,), y (n,), y_true(x) (n,)\n",
        "    \"\"\"\n",
        "    x = rng.uniform(x_min, x_max, size=n)\n",
        "    y_true = 0.5 * x**2 - 3.0 * x + 10.0\n",
        "    y = y_true + rng.normal(0.0, noise, size=n)\n",
        "    return x, y, y_true\n",
        "\n",
        "def make_classification_data(n=60, noise=0.25):\n",
        "    \"\"\"\n",
        "    Create 2-D classification data separable by a wiggly boundary.\n",
        "    The label is determined by sign of a smooth function + noise.\n",
        "    Returns: X (n,2), y (n,), plus a function handle f(X) for 'ideal' curve.\n",
        "    \"\"\"\n",
        "    X = rng.uniform(-1.0, 1.0, size=(n, 2))\n",
        "    f = lambda x0, x1: (2*x0**3 - x0) + np.sin(3*x1)\n",
        "    logits = f(X[:,0], X[:,1]) + rng.normal(0, noise, size=n)\n",
        "    y = (logits > 0).astype(int)\n",
        "    return X, y, f\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac804dc",
      "metadata": {
        "id": "bac804dc"
      },
      "source": [
        "## Interactive Demo — Regression (Polynomial + L2)\n",
        "\n",
        "Use the controls to:\n",
        "- Change **polynomial degree** (complexity).\n",
        "- Adjust **noise** (data uncertainty).\n",
        "- Add **L2 regularization** (λ).  \n",
        "- See train/test **MSE** and compare the **fit curve** to the (hidden) ideal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "754f1b35",
      "metadata": {
        "id": "754f1b35"
      },
      "outputs": [],
      "source": [
        "#@title Regression Playground\n",
        "degree = IntSlider(value=3, min=0, max=12, step=1, description='Degree')\n",
        "n_samples = IntSlider(value=30, min=10, max=200, step=5, description='Samples')\n",
        "noise = FloatSlider(value=10.0, min=0.0, max=50.0, step=1.0, description='Noise')\n",
        "lam = FloatSlider(value=0.0, min=0.0, max=100.0, step=1.0, description='λ (L2)')\n",
        "test_frac = FloatSlider(value=0.3, min=0.1, max=0.6, step=0.05, description='Test frac')\n",
        "\n",
        "def plot_regression(degree, n_samples, noise, lam, test_frac):\n",
        "    # 1) Data\n",
        "    x, y, y_true = make_regression_data(n=n_samples, noise=noise)\n",
        "    idx = rng.permutation(n_samples)\n",
        "    n_test = int(n_samples * test_frac)\n",
        "    test_idx, train_idx = idx[:n_test], idx[n_test:]\n",
        "    x_train, y_train = x[train_idx], y[train_idx]\n",
        "    x_test, y_test = x[test_idx], y[test_idx]\n",
        "\n",
        "    # 2) Pipeline: Poly -> Standardize -> Ridge\n",
        "    model = Pipeline([\n",
        "        (\"poly\", PolynomialFeatures(degree=degree, include_bias=True)),\n",
        "        (\"scale\", StandardScaler(with_mean=False)),\n",
        "        (\"ridge\", Ridge(alpha=lam, fit_intercept=True, random_state=0))\n",
        "    ])\n",
        "\n",
        "    X_train = x_train.reshape(-1, 1)\n",
        "    X_test  = x_test.reshape(-1, 1)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 3) Evaluate\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test  = model.predict(X_test)\n",
        "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "    mse_test  = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "    # 4) Plot\n",
        "    xx = np.linspace(x.min(), x.max(), 300).reshape(-1, 1)\n",
        "    y_fit = model.predict(xx)\n",
        "    y_ideal = 0.5 * xx[:,0]**2 - 3.0 * xx[:,0] + 10.0\n",
        "\n",
        "    plt.figure()\n",
        "    plt.scatter(x_train, y_train, label=\"train\", alpha=0.8)\n",
        "    plt.scatter(x_test,  y_test,  label=\"test\",  alpha=0.8)\n",
        "    plt.plot(xx, y_ideal, linestyle=\"--\", label=\"y_ideal\")\n",
        "    plt.plot(xx, y_fit, label=\"y_fit\")\n",
        "    plt.title(f\"Regression: degree={degree}, λ={lam:.1f} | MSE train={mse_train:.1f}, test={mse_test:.1f}\")\n",
        "    plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "interactive_output_reg = interactive_output(\n",
        "    plot_regression,\n",
        "    {\"degree\": degree, \"n_samples\": n_samples, \"noise\": noise, \"lam\": lam, \"test_frac\": test_frac}\n",
        ")\n",
        "\n",
        "display(VBox([HBox([degree, n_samples]), HBox([noise, lam, test_frac]), interactive_output_reg]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a765125f",
      "metadata": {
        "id": "a765125f"
      },
      "source": [
        "## Interactive Demo — Classification (Logistic Regression + Polynomial Features)\n",
        "\n",
        "Use the controls to:\n",
        "- Change **polynomial degree** (decision boundary complexity).\n",
        "- Adjust **noise** (how mixed the classes are).\n",
        "- Apply **L2 regularization** (C is inverse of strength; we report λ for intuition).\n",
        "\n",
        "We visualize the decision boundary and report **train/test accuracy** and **log loss**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27bf5f09",
      "metadata": {
        "id": "27bf5f09"
      },
      "outputs": [],
      "source": [
        "#@title Classification Playground\n",
        "degree_c = IntSlider(value=3, min=1, max=10, step=1, description='Degree')\n",
        "n_samples_c = IntSlider(value=80, min=30, max=400, step=10, description='Samples')\n",
        "noise_c = FloatSlider(value=0.25, min=0.0, max=1.0, step=0.05, description='Noise')\n",
        "lam_c = FloatSlider(value=1.0, min=0.0, max=50.0, step=0.5, description='λ (L2)')\n",
        "test_frac_c = FloatSlider(value=0.3, min=0.1, max=0.6, step=0.05, description='Test frac')\n",
        "\n",
        "def plot_classification(degree_c, n_samples_c, noise_c, lam_c, test_frac_c):\n",
        "    # 1) Data\n",
        "    X, y, f_ideal = make_classification_data(n=n_samples_c, noise=noise_c)\n",
        "    idx = rng.permutation(n_samples_c)\n",
        "    n_test = int(n_samples_c * test_frac_c)\n",
        "    test_idx, train_idx = idx[:n_test], idx[n_test:]\n",
        "    X_train, y_train = X[train_idx], y[train_idx]\n",
        "    X_test, y_test   = X[test_idx],  y[test_idx]\n",
        "\n",
        "    # 2) Model: poly -> standardize -> logistic regression (L2)\n",
        "    C = np.inf if lam_c == 0 else 1.0/lam_c\n",
        "    model = Pipeline([\n",
        "        (\"poly\", PolynomialFeatures(degree=degree_c, include_bias=True)),\n",
        "        (\"scale\", StandardScaler(with_mean=False)),\n",
        "        (\"logreg\", LogisticRegression(penalty=\"l2\", C=C, solver=\"lbfgs\", max_iter=500, fit_intercept=True))\n",
        "    ])\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    prob_train = model.predict_proba(X_train)[:,1]\n",
        "    prob_test  = model.predict_proba(X_test)[:,1]\n",
        "    y_pred_train = (prob_train >= 0.5).astype(int)\n",
        "    y_pred_test  = (prob_test  >= 0.5).astype(int)\n",
        "    acc_train = accuracy_score(y_train, y_pred_train)\n",
        "    acc_test  = accuracy_score(y_test,  y_pred_test)\n",
        "    ll_train  = log_loss(y_train, prob_train, labels=[0,1])\n",
        "    ll_test   = log_loss(y_test,  prob_test,  labels=[0,1])\n",
        "\n",
        "    # 3) Grid for boundary\n",
        "    x0_min, x1_min = X[:,0].min()-0.2, X[:,1].min()-0.2\n",
        "    x0_max, x1_max = X[:,0].max()+0.2, X[:,1].max()+0.2\n",
        "    gx0, gx1 = np.meshgrid(np.linspace(x0_min, x0_max, 200),\n",
        "                           np.linspace(x1_min, x1_max, 200))\n",
        "    grid = np.c_[gx0.ravel(), gx1.ravel()]\n",
        "    probs = model.predict_proba(grid)[:,1].reshape(gx0.shape)\n",
        "\n",
        "    # 4) Plot\n",
        "    plt.figure()\n",
        "    plt.contour(gx0, gx1, probs, levels=[0.5], linewidths=2)\n",
        "    plt.scatter(X_train[:,0], X_train[:,1], c=y_train, marker='o', label='train', alpha=0.85)\n",
        "    plt.scatter(X_test[:,0],  X_test[:,1],  c=y_test,  marker='s', label='test',  alpha=0.85)\n",
        "    plt.title(f\"Classification: degree={degree_c}, λ={lam_c:.2f} | acc train={acc_train:.2f}, test={acc_test:.2f}\")\n",
        "    plt.xlabel(\"x0\"); plt.ylabel(\"x1\"); plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "interactive_output_cls = interactive_output(\n",
        "    plot_classification,\n",
        "    {\"degree_c\": degree_c, \"n_samples_c\": n_samples_c, \"noise_c\": noise_c,\n",
        "     \"lam_c\": lam_c, \"test_frac_c\": test_frac_c}\n",
        ")\n",
        "\n",
        "display(VBox([HBox([degree_c, n_samples_c]), HBox([noise_c, lam_c, test_frac_c]), interactive_output_cls]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3f5286a",
      "metadata": {
        "id": "e3f5286a"
      },
      "source": [
        "## Practical Guidance (as in the course)\n",
        "\n",
        "Try the following experiments:\n",
        "- **Underfit**: Regression with degree = 1 (line) on quadratic data → high bias.\n",
        "- **Overfit**: Raise degree to 8–12 with few points → wiggly curve; low train error, higher test error.\n",
        "- **Outliers**: Increase `Noise` or manually add extreme values; note how the fit changes. Regularization helps.\n",
        "- **Regularization**: Increase λ to smooth the curve or boundary.\n",
        "- **Data size**: Increase `Samples` to reduce variance; overfitting often decreases.\n",
        "\n",
        "**Remember**: choose hyperparameters (degree, λ) using a **validation** set or cross-validation, not the test set."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}