{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asupraja3/ml-ng-notebooks/blob/main/Sigmoid_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96da427b",
      "metadata": {
        "id": "96da427b"
      },
      "source": [
        "\n",
        "# Optional Lab: Sigmoid Function & Logistic Regression\n",
        "\n",
        "*Adapted, self-contained notebook inspired by Andrew Ng's Machine Learning Specialization.*\n",
        "\n",
        "In this ungraded lab, you will:\n",
        "- Explore the **sigmoid** (logistic) function.\n",
        "- See how **logistic regression** uses the sigmoid to map a linear model into probabilities in \\([0,1]\\).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdcfad17",
      "metadata": {
        "id": "bdcfad17"
      },
      "source": [
        "\n",
        "## Tools\n",
        "\n",
        "We will use:\n",
        "- **NumPy** for scientific computing.\n",
        "- **Matplotlib** for plotting.\n",
        "- **scikit-learn** for a simple logistic regression model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48b6d510",
      "metadata": {
        "id": "48b6d510"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Setup ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# If running outside classic Jupyter, you might need: %matplotlib inline\n",
        "np.set_printoptions(precision=4, suppress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3cf0555",
      "metadata": {
        "id": "d3cf0555"
      },
      "source": [
        "\n",
        "## Sigmoid (Logistic) Function\n",
        "\n",
        "For classification, we want outputs between **0 and 1** (interpretable as probabilities).  \n",
        "The **sigmoid** function maps any real-valued input \\(z\\) to \\(g(z)\\in(0,1)\\):\n",
        "\n",
        "\\[\n",
        "g(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\]\n",
        "\n",
        "- As \\(z \\to -\\infty\\), \\(g(z) \\to 0\\)  \n",
        "- As \\(z \\to +\\infty\\), \\(g(z) \\to 1\\)\n",
        "\n",
        "Below, we implement it and try array/scalar inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "357ff4df",
      "metadata": {
        "id": "357ff4df"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Exponential demo (like in-course walkthrough) ---\n",
        "\n",
        "# Input is an array\n",
        "input_array = np.array([1, 2, 3])\n",
        "exp_array = np.exp(input_array)\n",
        "print(\"Input to exp:\", input_array)\n",
        "print(\"Output of exp:\", exp_array)\n",
        "\n",
        "# Input is a single number (scalar)\n",
        "input_val = 1.0\n",
        "exp_val = np.exp(input_val)\n",
        "print(\"Input to exp:\", input_val)\n",
        "print(\"Output of exp:\", exp_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e84434",
      "metadata": {
        "id": "a6e84434"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Sigmoid implementation ---\n",
        "def sigmoid(z):\n",
        "    \"\"\"Vectorized sigmoid that works for scalars, 1D, or ND arrays.\"\"\"\n",
        "    # For better numerical stability on large |z|, use a piecewise form:\n",
        "    z = np.array(z, dtype=float)\n",
        "    out = np.empty_like(z, dtype=float)\n",
        "    pos = z >= 0\n",
        "    neg = ~pos\n",
        "    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))\n",
        "    # For negative z, rewrite: 1/(1+e^-z) = e^z / (1+e^z)\n",
        "    ez = np.exp(z[neg])\n",
        "    out[neg] = ez / (1.0 + ez)\n",
        "    return out\n",
        "\n",
        "# Try sigmoid with array & scalar\n",
        "z_array = np.array([-6, -3, -1, 0, 1, 3, 6], dtype=float)\n",
        "print(\"z array:\", z_array)\n",
        "print(\"sigmoid(z array):\", sigmoid(z_array))\n",
        "\n",
        "z_scalar = -2.0\n",
        "print(\"z scalar:\", z_scalar, \" -> sigmoid(z):\", float(sigmoid(z_scalar)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2e1c9f7",
      "metadata": {
        "id": "e2e1c9f7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Plot the sigmoid curve ---\n",
        "z = np.linspace(-8, 8, 400)\n",
        "g = sigmoid(z)\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(z, g)\n",
        "plt.axvline(0, linestyle='--', linewidth=1)\n",
        "plt.axhline(0.5, linestyle='--', linewidth=1)\n",
        "plt.xlabel(\"z\")\n",
        "plt.ylabel(\"g(z)\")\n",
        "plt.title(\"Sigmoid / Logistic Function\")\n",
        "plt.ylim(-0.05, 1.05)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8e7787",
      "metadata": {
        "id": "1e8e7787"
      },
      "source": [
        "\n",
        "## Logistic Regression\n",
        "\n",
        "A logistic regression model applies the sigmoid to the familiar linear model:\n",
        "\n",
        "\\[\n",
        "f_{w,b}(x) = g(w^T x + b), \\quad \\text{where} \\quad g(z)=\\frac{1}{1+e^{-z}}.\n",
        "\\]\n",
        "\n",
        "- The term \\(z = w^T x + b\\) is a **linear regression** output.  \n",
        "- Passing \\(z\\) through **sigmoid** yields a **probability**: \\(P(y=1\\mid x)\\).  \n",
        "- The usual decision rule is: predict **1** if \\(f_{w,b}(x) \\ge 0.5\\), else **0**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eef21e78",
      "metadata": {
        "id": "eef21e78"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Small 1D classification dataset (tumor size -> malignant?) ---\n",
        "x_train = np.array([0., 1., 2., 3., 4., 5.])\n",
        "y_train = np.array([0, 0, 0, 1, 1, 1])\n",
        "\n",
        "# Reshape to (m, 1) for scikit-learn\n",
        "X = x_train.reshape(-1, 1)\n",
        "print(\"X shape:\", X.shape, \"y shape:\", y_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee22cba",
      "metadata": {
        "id": "aee22cba"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Fit logistic regression and inspect ---\n",
        "logreg = LogisticRegression(solver='lbfgs')\n",
        "logreg.fit(X, y_train)\n",
        "\n",
        "# Probability for class 1\n",
        "proba = logreg.predict_proba(X)[:, 1]\n",
        "y_pred = (proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"w (coef):\", logreg.coef_, \"b (intercept):\", logreg.intercept_)\n",
        "print(\"Accuracy:\", accuracy_score(y_train, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_train, y_pred))\n",
        "print(classification_report(y_train, y_pred, digits=3))\n",
        "\n",
        "# Plot points + probability curve\n",
        "xs = np.linspace(x_train.min() - 0.5, x_train.max() + 0.5, 200).reshape(-1, 1)\n",
        "probs = logreg.predict_proba(xs)[:, 1]\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.scatter(x_train[y_train==0], y_train[y_train==0], marker='o', label='y=0')\n",
        "plt.scatter(x_train[y_train==1], y_train[y_train==1], marker='x', label='y=1')\n",
        "plt.plot(xs, probs, label='P(y=1 | x)')\n",
        "plt.axhline(0.5, linestyle='--', linewidth=1, label='0.5 threshold')\n",
        "plt.xlabel(\"Tumor size (x)\")\n",
        "plt.ylabel(\"Probability / label\")\n",
        "plt.title(\"Logistic Regression on Categorical Data\")\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "046d334a",
      "metadata": {
        "id": "046d334a"
      },
      "source": [
        "\n",
        "## Notes / Try This\n",
        "\n",
        "- Notice the orange line in the lecture corresponds to \\(z = w^T x + b\\), which is **not** a probability by itself.  \n",
        "- The sigmoid wraps \\(z\\) into \\([0,1]\\), giving a probability curve.  \n",
        "- These predictions match the pattern that larger tumors (right side) are more likely to be malignant (label 1).\n",
        "\n",
        "**Exercises:**\n",
        "1. Add additional data points near `x=9` or `x=10` (large size) and re-run the notebook. What happens to the curve and predictions?  \n",
        "2. Change the decision threshold (e.g., 0.3 or 0.7) and observe the effect on predictions.  \n",
        "3. Compare against a linear regression model with a 0.5 threshold and note the differences in behavior.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}